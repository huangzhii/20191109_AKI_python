5 fold CV -- 1/5
5 fold CV -- 2/5
5 fold CV -- 3/5
5 fold CV -- 4/5
5 fold CV -- 5/5
Current model: GBM, current hyper-parameter: 8, current F1 score: 0.73578550; current sensitivity: 0.81520208
5 fold CV -- 1/5
5 fold CV -- 2/5
5 fold CV -- 3/5
5 fold CV -- 4/5
5 fold CV -- 5/5
Current model: GBM, current hyper-parameter: 16, current F1 score: 0.71684373; current sensitivity: 0.72097616
5 fold CV -- 1/5
5 fold CV -- 2/5
5 fold CV -- 3/5
5 fold CV -- 4/5
5 fold CV -- 5/5
Current model: GBM, current hyper-parameter: 32, current F1 score: 0.65829597; current sensitivity: 0.53369176
5 fold CV -- 1/5
5 fold CV -- 2/5
5 fold CV -- 3/5
5 fold CV -- 4/5
5 fold CV -- 5/5
Current model: GBM, current hyper-parameter: 64, current F1 score: 0.58046711; current sensitivity: 0.34614963
5 fold CV -- 1/5
5 fold CV -- 2/5
5 fold CV -- 3/5
5 fold CV -- 4/5
5 fold CV -- 5/5
Current model: GBM, current hyper-parameter: 128, current F1 score: 0.48373323; current sensitivity: 0.18539495
5 fold CV -- 1/5
5 fold CV -- 2/5
5 fold CV -- 3/5
5 fold CV -- 4/5
5 fold CV -- 5/5
Current model: GBM, current hyper-parameter: 256, current F1 score: 0.38219879; current sensitivity: 0.06689111
Best hyper-parameter: 8
Model:
GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='deviance', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=8,
                           n_iter_no_change=None, presort='deprecated',
                           random_state=None, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
[MIMIC] train AUC: 0.94423348, test AUC: 0.82403856, train F1: 0.83523805, test F1: 0.74636099, test Precision: 0.75738769, test Recall: 0.76901472, sensitivity: 0.86570296, specificity: 0.67232648, PPV: 0.62793844, NPV: 0.88683694
