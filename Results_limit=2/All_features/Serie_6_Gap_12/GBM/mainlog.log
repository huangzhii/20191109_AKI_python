5 fold CV -- 1/5
5 fold CV -- 2/5
5 fold CV -- 3/5
5 fold CV -- 4/5
5 fold CV -- 5/5
Current model: GBM, current hyper-parameter: 8, current F1 score: 0.57099602; current sensitivity: 0.39335850
5 fold CV -- 1/5
5 fold CV -- 2/5
5 fold CV -- 3/5
5 fold CV -- 4/5
5 fold CV -- 5/5
Current model: GBM, current hyper-parameter: 16, current F1 score: 0.63465836; current sensitivity: 0.46014779
5 fold CV -- 1/5
5 fold CV -- 2/5
5 fold CV -- 3/5
5 fold CV -- 4/5
5 fold CV -- 5/5
Current model: GBM, current hyper-parameter: 32, current F1 score: 0.39779385; current sensitivity: 0.06936027
5 fold CV -- 1/5
5 fold CV -- 2/5
5 fold CV -- 3/5
5 fold CV -- 4/5
5 fold CV -- 5/5
Current model: GBM, current hyper-parameter: 64, current F1 score: 0.39185263; current sensitivity: 0.05063080
5 fold CV -- 1/5
5 fold CV -- 2/5
5 fold CV -- 3/5
5 fold CV -- 4/5
5 fold CV -- 5/5
Current model: GBM, current hyper-parameter: 128, current F1 score: 0.34251834; current sensitivity: 0.00000000
5 fold CV -- 1/5
5 fold CV -- 2/5
5 fold CV -- 3/5
5 fold CV -- 4/5
5 fold CV -- 5/5
Current model: GBM, current hyper-parameter: 256, current F1 score: 0.34291536; current sensitivity: 0.00000000
Best hyper-parameter: 16
Model:
GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='deviance', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=16,
                           n_iter_no_change=None, presort='deprecated',
                           random_state=None, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
[MIMIC] train AUC: 0.98837137, test AUC: 0.86352065, train F1: 0.94627479, test F1: 0.58111718, test Precision: 0.69710298, test Recall: 0.64290723, sensitivity: 0.34959103, specificity: 0.93622343, PPV: 0.88293803, NPV: 0.51126793
