5 fold CV -- 1/5
5 fold CV -- 2/5
5 fold CV -- 3/5
5 fold CV -- 4/5
5 fold CV -- 5/5
Current model: GBM, current hyper-parameter: 8, current F1 score: 0.74294681; current sensitivity: 0.89710043
5 fold CV -- 1/5
5 fold CV -- 2/5
5 fold CV -- 3/5
5 fold CV -- 4/5
5 fold CV -- 5/5
Current model: GBM, current hyper-parameter: 16, current F1 score: 0.75694084; current sensitivity: 0.85033804
5 fold CV -- 1/5
5 fold CV -- 2/5
5 fold CV -- 3/5
5 fold CV -- 4/5
5 fold CV -- 5/5
Current model: GBM, current hyper-parameter: 32, current F1 score: 0.76080049; current sensitivity: 0.76995052
5 fold CV -- 1/5
5 fold CV -- 2/5
5 fold CV -- 3/5
5 fold CV -- 4/5
5 fold CV -- 5/5
Current model: GBM, current hyper-parameter: 64, current F1 score: 0.72216736; current sensitivity: 0.62538972
5 fold CV -- 1/5
5 fold CV -- 2/5
5 fold CV -- 3/5
5 fold CV -- 4/5
5 fold CV -- 5/5
Current model: GBM, current hyper-parameter: 128, current F1 score: 0.67074950; current sensitivity: 0.47695087
5 fold CV -- 1/5
5 fold CV -- 2/5
5 fold CV -- 3/5
5 fold CV -- 4/5
5 fold CV -- 5/5
Current model: GBM, current hyper-parameter: 256, current F1 score: 0.57061505; current sensitivity: 0.29449516
Best hyper-parameter: 32
Model:
GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='deviance', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_impurity_split=None,
                           min_samples_leaf=1, min_samples_split=2,
                           min_weight_fraction_leaf=0.0, n_estimators=32,
                           n_iter_no_change=None, presort='deprecated',
                           random_state=None, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
[MIMIC] train AUC: 0.94905648, test AUC: 0.86630076, train F1: 0.86990275, test F1: 0.78496588, test Precision: 0.78405216, test Recall: 0.78790602, sensitivity: 0.79597315, specificity: 0.77983889, PPV: 0.73592767, NPV: 0.83217666
